---
permalink: /
title: "Mason Sawtell"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Data Scientist currently at Neudesic, an IBM Company and am primarily working with NLP models for compute/data-constrained use cases as well as explainability. I have written several papers related to these topics, and have collaborated with IBM and Microsoft employees to help improve their own models for content safety and entailment (groundedness). Recently, I have been interested in various techniques to explain a transformer model's output and am working on ways to apply it at a large scale.

I have a Bachelor's degree in Data Science from the University of California, Irvine, where I worked at the Office of Research Information and helped perform research in the Department of Neurobiology and Behavior. From those experiences, I learned a lot about the process of research (especially in the health sciences) and it helped spark my interest in performing my own research.

While explainability is important for any machine learning system, I also believe that it cannot mitigate all potential harm from a model. Effort must be taken to truly understand the impact of a system, as well as how that impact is distributed across groups. Machine learning models are not developed in a vacuum, and can often perpetuate inequalities present in the real world without direct effort to improve fairness. In some cases, automating decisions using machine learning can harm the communities it outwardly appears to help, as in the case of [diagnosing illnesses](https://www.rollingstone.com/culture/culture-features/ai-health-care-patient-safety-privacy-1235006118/). Professor Emily M. Bender highlights this idea perfectly when she urges people to consider *what* decision is being automated and *why*.

Outside of data science, I love to read (both fiction and nonfiction), cook, bake, and exercise!

<img src="../images/bread.jpg" width="300" height="300"/> <img src="../images/pie.jpg" width="300" height="300"/>