---
layout: single
title:  "Draft Post"
header:
  teaser: "unsplash-gallery-image-2-th.jpg"
categories: 
  - Jekyll
tags:
  - edge case
---
While explainability is important for any machine learning system, I also believe that it cannot mitigate all potential harm from a model. Effort must be taken to truly understand the impact of a system, as well as how that impact is distributed across groups. Machine learning models are not developed in a vacuum, and can often perpetuate inequalities present in the real world without direct effort to improve fairness. In some cases, automating decisions using machine learning can harm the communities it outwardly appears to help, as in the case of [diagnosing illnesses](https://www.rollingstone.com/culture/culture-features/ai-health-care-patient-safety-privacy-1235006118/). Professor Emily M. Bender highlights this idea perfectly when she urges people to consider *what* decision is being automated and *why*.